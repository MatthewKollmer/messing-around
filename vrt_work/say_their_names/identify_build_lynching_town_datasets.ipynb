{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Lynching Town Newspapers\n",
    "\n",
    "This notebook outlines my steps in finding newspapers from towns where lynchings occurred and scraping Chron Am to build datasets of those newspapers. "
   ],
   "id": "788b006dd89861a4"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import pandas as pd\n",
    "from geopy.geocoders import Nominatim\n",
    "import folium\n",
    "import time\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import timedelta\n",
    "from datetime import datetime"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1) Find localities in our data where lynchings occurred.",
   "id": "8af74db6493c4c68"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "seguin_rigby_df = pd.read_csv('seguin_rigby_data_black_subset_02.csv')",
   "id": "8d67bfbdc1e0cabd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "I create a 'place' column in the data so we can review city and state at once.",
   "id": "131dfb7fd794052"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "seguin_rigby_df['place'] = seguin_rigby_df['city'] + ', ' + seguin_rigby_df['state']",
   "id": "a479712334b2d475",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "To review the number of lynchings per place in our data, I use .value_counts(). I thought this would be the best way to find lynching towns, but not so much. Still, it's helpful to see the relative rates by place.",
   "id": "a21e82b6000baddb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "seguin_rigby_df['place'].value_counts()",
   "id": "f5a32814480f4abe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2) Geolocate lynching towns.\n",
    "\n",
    "I used geopy and Nomatim to find lat/long data for towns. The results are mostly good, but a fair number of mis-located places (see map below). Nomatim doesn't seem to be as accurate as using Chron Am's place metadata, but then, I wouldn't know exactly how to cross-reference that metadata with the lynchings... Anyway, these libraries worked fine for now."
   ],
   "id": "534214925bf66d50"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "geolocator = Nominatim(user_agent='its_me')\n",
    "\n",
    "def get_lat_long(place):\n",
    "    try:\n",
    "        location = geolocator.geocode(place)\n",
    "        if location:\n",
    "            return location.latitude, location.longitude\n",
    "        else:\n",
    "            return None, None\n",
    "    except Exception as e:\n",
    "        return None, None\n",
    "\n",
    "seguin_rigby_df['latitude'], seguin_rigby_df['longitude'] = zip(*seguin_rigby_df['place'].apply(get_lat_long))\n",
    "\n",
    "time.sleep(1)"
   ],
   "id": "c658b57e9c789f44",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "seguin_rigby_df",
   "id": "de7b1061bc3df25",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Note to self: add updated seguin_rigby_data_black_subset_02.csv to GitHub.",
   "id": "b42deb91b162b9e6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "seguin_rigby_df.to_csv('seguin_rigby_data_black_subset_02.csv')",
   "id": "87948d8f91ac8f7d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "I create a map of all the lynchings in our subset of the data:",
   "id": "3385a1bdbb292caf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "map_start_point = [39.8283, -98.5795]\n",
    "\n",
    "map = folium.Map(location=map_start_point, tiles=\"Cartodb Positron\", zoom_start=4)\n",
    "\n",
    "for index, row in seguin_rigby_df.iterrows():\n",
    "\n",
    "    if np.isnan(row['latitude']) or np.isnan(row['longitude']):\n",
    "        continue\n",
    "\n",
    "    tooltip = f\"<div style='font-size: 11pt'>{row['victim']}</div>\" \\\n",
    "              f\"<div style='font-size: 11pt'>{row['place']}</div>\" \\\n",
    "              f\"<div style='font-size: 11pt'>{row['year']}</div>\"\n",
    "\n",
    "    folium.Circle(\n",
    "        [row['latitude'], row['longitude']],\n",
    "        tooltip=tooltip,\n",
    "        color='darkred',\n",
    "        radius=10\n",
    "    ).add_to(map)\n",
    "\n",
    "map"
   ],
   "id": "9f398e70af323504",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "So, this was intriguing. Why are there virtually no lynchings in the Southern states? Seguin/Rigby's article claims the South was the central location of Black lynchings in their data, but in our subset, this is not the case. Is it because we subset by 1883 to 1921? And by \"victim's name known\"? I will have to look over the full Seguin/Rigby dataset again. Just seems strange...\n",
    "\n",
    "You know what, I think this could be a significant and meaningful observation. If Southern lynchings typically did not leave records of victim's names, it says something about the attitude toward those victims. There's a certain level of humanity granted to naming the victim. It would be a significant erasure."
   ],
   "id": "320b97f71f716324"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3) Identify Lynching Towns with Newspapers\n",
    "\n",
    "I haven't come up with a way to do this programmatically. I'd need all the geolocations of the newspapers in Chron Am and their coverage dates. Idk. Could be done, but rather than spend time figuring it out, I've been looking for candidates by hand by cross-referencing my map of lynching incidents to Chron Am's map of digitized newspapers ([__found here__](https://loc.maps.arcgis.com/apps/instant/media/index.html?appid=3c6a392554d545bdb1c083348ef56458&center=-97.5126;39.6376&level=3)). I thought this would be easier than it is... But the limitations of Chron Am's data reveals itself here in ways you don't realize looking at the data as rows or lists. That is, the map makes you see the gaps in spaces where surely there were newspapers, but they are not digitized or available. What this means is it is surprisingly difficult to find towns where lynchings occurred AND where there is available digitized newspaper data.\n",
    "\n",
    "That being said, after a couple hours cross-referencing the maps by hand, I've found a few:\n",
    "\n",
    "__Newspaper:__ Peninsula Enterprise, Acconomac, VA <br>\n",
    "Page: https://chroniclingamerica.loc.gov/lccn/sn94060041/ <br>\n",
    "__Incident:__ Magruder Fletcher, Tasley, VA <br>\n",
    "A candidate because there is coverage before, during, and after the incident (1889). Acconomac and Tasley are only a couple miles apart. If the Peninsula Enterprise covered local issues at all, it would have likely covered happenings in Tasley. The Magruder Fletcher csv has 28 hits, too, making it a fairly widely reported incident.\n",
    "<br>\n",
    "<br>\n",
    "__Newspaper:__ Maryland Independent, La Plata, MD <br>\n",
    "Page: https://chroniclingamerica.loc.gov/lccn/sn85025407/ <br>\n",
    "__Incident:__ Joseph Cocking, Port Tobacco, MD <br>\n",
    "Another good candidate. There is coverage before, during, and after the incident (1896). La Plata and Port Tobacco are only about 3 miles from each other. The Joseph Cocking csv has 70 hits, making it a widely reported incident.\n",
    "<br>\n",
    "<br>\n",
    "__Newspaper:__ Lexington Intelligencer, Lexington, MO <br>\n",
    "Page: https://chroniclingamerica.loc.gov/lccn/sn86063623/ <br>\n",
    "__Incident:__ Harry Gates, Lexington, MO <br>\n",
    "Another good candidate. There's also the Weekly Intelligencer which published the year prior to the Harry Gates murder (1902), but not the year of.There are 101 hits in the Harry Gates csv, too. I'm going to just use the Lexington Intelligencer for now, but if you want to add the Weekly Intelligencer later, here it is: https://chroniclingamerica.loc.gov/lccn/sn93060416/\n"
   ],
   "id": "f34e97dc00a9a5b2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4) Build Lynching Town Newspaper Subsets\n",
    "\n",
    "I scraped Chron Am for the above newspapers. I pulled two years before the incident, the year of the incident, and the year after. I chose this range after double-checking it was covered by each paper (that each paper actually had digitized pages for their respective years surrounding the lynchings). I'm not sure if a four-year range is adequate. It's based on nothing but vibes. If I need more data, I'll have to go back and get it, but that's okay.\n",
    "\n",
    "I saved the results in a new directory called lynching_town_newspapers. The files are:\n",
    "\n",
    "- peninsula_enterprise_1888-90.csv (associated with Magruder Fletcher)\n",
    "- maryland_independent_1894-97.csv (associated with Joseph Cocking)\n",
    "- lexington_intelligencer_1899-1903.csv (associated with Harry Gates)\n",
    "\n",
    "To do this, I used scraping code I'd written last year as part of another project. It is different than the scrape_carefully() function I've used elsewhere in this project. It starts with setting parameters. It's important to double-check these details with Chron Am (i.e., see how many pages per paper, that your date ranges are covered by Chron Am, etc). But then, you can enter the parameters in these objects:"
   ],
   "id": "1bbfb9511a92c868"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "target_paper = {\n",
    "    'sn86063623': ('Lexington Intelligencer', 'Lexington', 'MO', 'harry gates')\n",
    "}\n",
    "\n",
    "START_DATE = datetime(1899, 1, 1)\n",
    "END_DATE = datetime(1903, 12, 31)\n",
    "DATE_FORMAT = \"%Y-%m-%d\"\n",
    "START_PAGE = 1\n",
    "END_PAGE = 9\n",
    "iterating_date = START_DATE"
   ],
   "id": "f6384a9ac455d91f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Then create some necessary shells and functions:",
   "id": "f8080b1dd1e1997b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df = pd.DataFrame(columns=['url', 'text', 'date', 'newspaper', 'city', 'state', 'victim_association'])\n",
    "\n",
    "def pull_row(data, new_row):\n",
    "    data.loc[len(data)] = new_row"
   ],
   "id": "b950debc2d211eae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "And finally, loop through the possible url combinations for the newspaper:",
   "id": "a231cb58ecafeac9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "hold_up_wait = 10\n",
    "\n",
    "while iterating_date <= END_DATE:\n",
    "\n",
    "    formatted_date = iterating_date.strftime(DATE_FORMAT)\n",
    "\n",
    "    for sn_code, (newspaper, city, state, victim) in target_paper.items():\n",
    "\n",
    "        consistent_url = f'https://chroniclingamerica.loc.gov/lccn/{sn_code}/'\n",
    "\n",
    "        for page in range(START_PAGE, END_PAGE):\n",
    "\n",
    "            url_string = f'{consistent_url}{formatted_date}/ed-1/seq-{page}/ocr/'\n",
    "\n",
    "            print(url_string)\n",
    "\n",
    "            try:\n",
    "\n",
    "                pulled_data = requests.get(url_string, timeout=hold_up_wait)\n",
    "\n",
    "                if pulled_data.status_code == 200:\n",
    "                    soup = BeautifulSoup(pulled_data.content, 'lxml')\n",
    "                    text_chunks = soup.find_all('p')\n",
    "                    text = ' '.join([p.get_text() for p in text_chunks])\n",
    "                    pull_row(df, [url_string, text, formatted_date, newspaper, city, state, victim])\n",
    "\n",
    "            except requests.exceptions.Timeout:\n",
    "                print(f\"Timeout occurred for URL: {url_string}\")\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"An error occurred: {e}\")\n",
    "\n",
    "    iterating_date += timedelta(days=1)"
   ],
   "id": "f3ae15097b317891",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df",
   "id": "bdc3f48a5da76ed4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.to_csv('lexington_intelligencer_1899-1903.csv')",
   "id": "14ada6d8f0c044fe",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

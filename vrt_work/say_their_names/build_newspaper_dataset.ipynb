{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Scraping Chron Am for Black Victim Clusters\n",
    "\n",
    "This notebook outlines several steps toward identifying newspaper reports about Black lynching victims. It relies on the dataset published by Seguin & Rigby. To learn more, read here: https://journals.sagepub.com/doi/pdf/10.1177/2378023119841780\n",
    "\n",
    "See my preprocessing notebook to understand how I got the Chron Am search results. It's important to note that these search results are not exact. They will give you every instance where the victim's name appears in newspaper issues from the year documented AND when the city name where the lynching occurred is printed within 100 words of the victim name. In other words, there may be instances where a coincidentally identical name and city appears in ChronAm, but are not related to the lynching event."
   ],
   "id": "555b8ba65fc99249"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "e52b0537b5d9291e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import os"
   ],
   "id": "2691109dab09a7a2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df = pd.read_csv('seguin_rigby_data_black_subset.csv')",
   "id": "8d3fa09eff012469",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df",
   "id": "d599f9729d9b9837",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df['search_url']",
   "id": "d452344f91279d49",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# this Regex pattern will help to scrape urls that direct to search hits\n",
    "# it is used in the scrape_carefully() function below\n",
    "page_pattern = re.compile(r'/lccn/sn\\d+/\\d{4}-\\d{2}-\\d{2}/ed-\\d/seq-\\d+/')\n",
    "\n",
    "# these are presets to help keep track of time it takes to scrape and the request count\n",
    "request_count = 0\n",
    "first_request_time = None"
   ],
   "id": "69be373d8b8932f5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# this function scrapes ChronAm, but keeps track of requests and chills itself to avoid hitting their rate limits\n",
    "def scrape_carefully(url, retries=3):\n",
    "    global request_count, first_request_time\n",
    "    \n",
    "    if request_count == 0:\n",
    "        first_request_time = datetime.now()\n",
    "    \n",
    "    if request_count >= 200:\n",
    "        elapsed_time = datetime.now() - first_request_time\n",
    "        \n",
    "        if elapsed_time < timedelta(minutes=1):\n",
    "            print('Crawl limit reached. Waiting for 5 minutes.')\n",
    "            time.sleep(300)\n",
    "            first_request_time = datetime.now()\n",
    "            request_count = 0\n",
    "    \n",
    "    if request_count > 0 and request_count % 10 == 0:\n",
    "        print('Burst limit reached. Waiting for 10 seconds.')\n",
    "        time.sleep(10)\n",
    "    \n",
    "    for i in range(retries):\n",
    "        response = requests.get(url)\n",
    "        request_count += 1\n",
    "        print(f'Requests made: {request_count}')\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            return response\n",
    "        \n",
    "        elif response.status_code == 429:\n",
    "            print(f'Received 429 error. Sorry ChronAm. Waiting one hour.')\n",
    "            time.sleep(3605) # this is as safe as can be. Consider shortening chill time.\n",
    "            \n",
    "        else:\n",
    "            print(f'Unexpected error for {url}: {response.status_code}')\n",
    "            return None\n",
    "        \n",
    "    return None"
   ],
   "id": "b14b5a7e62ad1c10",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# this sets the timer to NOW. And you're off to the races!\n",
    "start_time = datetime.now()\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    search_url = row['search_url']\n",
    "    victim_name = row['victim'].replace(' ', '_')\n",
    "    scrape_results = []\n",
    "    \n",
    "    scrape_content = scrape_carefully(search_url)\n",
    "    \n",
    "    if scrape_content is None:\n",
    "        print(f'Retried 3 times but got repeated errors. Skipping search for victim {victim_name}')\n",
    "        continue\n",
    "    \n",
    "    soup = BeautifulSoup(scrape_content.text, 'html.parser')\n",
    "    \n",
    "    results_list = soup.find('ul', class_='results_list')\n",
    "    \n",
    "    if results_list is None:\n",
    "        print(f'No results found for victim {victim_name}')\n",
    "        continue\n",
    "    \n",
    "    matching_links = results_list.find_all('a', href=page_pattern)\n",
    "    \n",
    "    for link in matching_links:\n",
    "        link_text = link.get_text(strip=True)\n",
    "        match = page_pattern.search(link['href'])\n",
    "        if match:\n",
    "            matched_href = match.group()\n",
    "            link_href = f'https://chroniclingamerica.loc.gov{matched_href}ocr/'\n",
    "            scrape_results.append({'Link Title': link_text, 'URL': link_href})\n",
    "    \n",
    "    print(f'Search for victim {victim_name} processed.')\n",
    "    \n",
    "    if scrape_results:\n",
    "        df_results = pd.DataFrame(scrape_results)\n",
    "        csv_filename = f'lynch_clusters/{victim_name}.csv'\n",
    "        df_results.to_csv(csv_filename, index=False)\n",
    "        print(f'Results for victim {victim_name} saved to {csv_filename}')\n",
    "\n",
    "# A little added to keep track of total time elapsed\n",
    "end_time = datetime.now()\n",
    "total_elapsed_time = end_time - start_time\n",
    "print(f'Total elapsed time: {total_elapsed_time}')"
   ],
   "id": "b3b1ccd4183ea234",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now I have all the pages with hits for every name! Hehehehehe\n",
    "\n",
    "The code below does the next level of scraping. It uses scrape_carefully() to get the newspaper text."
   ],
   "id": "2844daf9e764a4f8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "request_count = 0\n",
    "start_time = datetime.now()\n",
    "\n",
    "directory = 'lynch_clusters/'\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.csv'):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        \n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        newspaper_content = []\n",
    "        \n",
    "        for url in df['URL']:\n",
    "            try:\n",
    "                response = scrape_carefully(url)\n",
    "                \n",
    "                if response and response.status_code == 200:\n",
    "                    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                    \n",
    "                    p_tags = soup.find_all('p')\n",
    "                    p_text = ' '.join([tag.get_text(strip=True) for tag in p_tags])\n",
    "                    \n",
    "                    newspaper_content.append(p_text)\n",
    "                    \n",
    "                else:\n",
    "                    newspaper_content.append(None)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f'Error scraping {url}: {e}')\n",
    "                newspaper_content.append(None)\n",
    "        \n",
    "        df['text'] = newspaper_content\n",
    "        df['text'] = df['text'].str.lower()\n",
    "        \n",
    "        df.to_csv(file_path, index=False)\n",
    "        print(f'Updated {filename} with OCR text.')\n",
    "\n",
    "end_time = datetime.now()\n",
    "total_elapsed_time = end_time - start_time\n",
    "print(f'Total elapsed time: {total_elapsed_time}')"
   ],
   "id": "a509ffbc06382178",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "ed90811391c2fd19",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6049e15aeabf11f",
   "metadata": {},
   "source": [
    "# Code for Adding Latitude and Longitude and Mapping Reprints\n",
    "\n",
    "The following notebook enriches lynch_clusters with longitude and latitude data for the cities of their reports. I also put together interactive maps for the clusters. These maps show location of reprints, the newspaper and its city, the victim name, and the clippings.\n",
    "\n",
    "Steps in this notebook:\n",
    "1) Review Viral Texts' place metadata.\n",
    "2) Get sn codes for lynch_clusters (extracted from 'URL' column)\n",
    "3) Match sn codes with instances in 'series.csv' then extract 'coverage' data\n",
    "4) Match coverage with instances in 'places.csv' then extract latitudinal and longitudanl data\n",
    "5) Map the data with folium.\n",
    "6) Display the data based on user selection using ipywidgets."
   ]
  },
  {
   "cell_type": "code",
   "id": "c46a199560de85a5",
   "metadata": {},
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "import folium\n",
    "from folium.plugins import MarkerCluster\n",
    "import webbrowser\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1e0f46e778a912ed",
   "metadata": {},
   "source": [
    "1) review Viral Texts' place metadata:"
   ]
  },
  {
   "cell_type": "code",
   "id": "d86f1cbab81bfa8a",
   "metadata": {},
   "source": [
    "lat_long_df = pd.read_csv('https://raw.githubusercontent.com/ViralTexts/newspaper-metadata/main/places.csv')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c7df40281c82eb04",
   "metadata": {},
   "source": [
    "series_df = pd.read_csv('https://raw.githubusercontent.com/ViralTexts/newspaper-metadata/main/series.csv')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1d731bd1bf2de5bf",
   "metadata": {},
   "source": [
    "lat_long_df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bb74dd625febe6fa",
   "metadata": {},
   "source": [
    "series_df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "64c505f28d4b6b62",
   "metadata": {},
   "source": [
    "2) Get sn codes for lynch_clusters (extracted from 'URL' column):"
   ]
  },
  {
   "cell_type": "code",
   "id": "c8e1e3932742a3fe",
   "metadata": {},
   "source": [
    "# extract sn codes from URL column\n",
    "\n",
    "directory = '/Users/matthewkollmer/PycharmProjects/nlp_research/vrt_work/lynch_clusters_02_refined'\n",
    "\n",
    "sn_code_regex = re.compile(r'sn\\d{8}')\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    file_path = os.path.join(directory, filename)\n",
    "        \n",
    "    df = pd.read_csv(file_path)\n",
    "        \n",
    "    sn_codes = []\n",
    "        \n",
    "    for url in df['URL']:\n",
    "        if pd.isna(url):\n",
    "            sn_codes.append(None)\n",
    "                \n",
    "        else:\n",
    "            sn_match = sn_code_regex.search(url)\n",
    "                \n",
    "            if sn_match:\n",
    "                sn_codes.append(sn_match.group(0))\n",
    "                    \n",
    "            else:\n",
    "                sn_codes.append(None)\n",
    "        \n",
    "    df['sn_code'] = sn_codes\n",
    "    \n",
    "    df['sn_code'] = df['sn_code'].fillna('')\n",
    "    df['sn_code'] = df['sn_code'].apply(lambda x: '/lccn/' + x if x else None)\n",
    "        \n",
    "    df.to_csv(file_path, index=False)\n",
    "    print(f'Updated {filename} with sn codes.')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "acb14d2886fb57fd",
   "metadata": {},
   "source": [
    "3) Match sn codes with instances in 'series.csv' then extract 'coverage' data:"
   ]
  },
  {
   "cell_type": "code",
   "id": "94a85b7fdf9d26da",
   "metadata": {},
   "source": [
    "# match sn codes between 'series' dataframe and victim csv files\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    file_path = os.path.join(directory, filename)\n",
    "        \n",
    "    df = pd.read_csv(file_path)\n",
    "        \n",
    "    df['coverage'] = None\n",
    "        \n",
    "    for i, sn_code in series_df['series'].items():\n",
    "\n",
    "        matching_sn_codes = df['sn_code'] == sn_code\n",
    "            \n",
    "        df.loc[matching_sn_codes, 'coverage'] = series_df.loc[i, 'coverage']\n",
    "        \n",
    "    df.to_csv(file_path, index=False)\n",
    "    print(f'Updated {filename} with coverage links.')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# add reprint date columns\n",
    "\n",
    "date_regex = re.compile(r'\\d{4}-\\d{2}-\\d{2}')\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    file_path = os.path.join(directory, filename)\n",
    "        \n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    reprint_dates = []\n",
    "    \n",
    "    for url in df['URL']:\n",
    "        if pd.isna(url):\n",
    "            reprint_dates.append(None)\n",
    "        else:\n",
    "            date_match = date_regex.search(url)\n",
    "            \n",
    "            if date_match:\n",
    "                reprint_dates.append(date_match.group(0))\n",
    "            else:\n",
    "                reprint_dates.append(None)\n",
    "    \n",
    "    df['reprint_date'] = reprint_dates\n",
    "    \n",
    "    df['reprint_date'] = pd.to_datetime(df['reprint_date'], format='%Y-%m-%d', errors='coerce')\n",
    "    \n",
    "    df.to_csv(file_path, index=False)\n",
    "    \n",
    "    print(f'Updated {filename} with reprint dates.')"
   ],
   "id": "b69af1d370dbb4ff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# add Newspaper Title column\n",
    "\n",
    "paper_title_regex = re.compile(r'^[\\w*\\s.]*\\w')\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    file_path = os.path.join(directory, filename)\n",
    "    \n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    paper_titles = []\n",
    "    \n",
    "    for url in df['Link Title']:\n",
    "        if pd.isna(url):\n",
    "            paper_titles.append(None)\n",
    "        else:\n",
    "            paper_match = paper_title_regex.search(url)\n",
    "            \n",
    "            if paper_match:\n",
    "                paper_titles.append(paper_match.group(0))\n",
    "            else:\n",
    "                paper_titles.append(None)\n",
    "    \n",
    "    df['newspaper'] = paper_titles\n",
    "    \n",
    "    df.to_csv(file_path, index=False)\n",
    "    \n",
    "    print(f'Updated {filename} with newspaper titles.')"
   ],
   "id": "b12d29e789e1cd56",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "test_df = pd.read_csv('/Users/matthewkollmer/PycharmProjects/nlp_research/vrt_work/lynch_clusters_02_refined/aaron_thomas.csv')\n",
    "\n",
    "test_df"
   ],
   "id": "cefb5a58879238c5",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a62d8497bca6f2ce",
   "metadata": {},
   "source": [
    "4) Match coverage with instances in 'places.csv' then extract latitudinal and longitudanl data:"
   ]
  },
  {
   "cell_type": "code",
   "id": "5baa65ad422995a7",
   "metadata": {},
   "source": [
    "# match 'coverage' column in victim csv files to 'coverage' column in 'places.csv'/lat_long_df and transfer lat/long data to victim csv files\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    file_path = os.path.join(directory, filename)\n",
    "        \n",
    "    df = pd.read_csv(file_path)\n",
    "        \n",
    "    df['longitude'] = None\n",
    "    df['latitude'] = None\n",
    "        \n",
    "    for i, coverage_link in lat_long_df['coverage'].items():\n",
    "   \n",
    "        matching_rows = df['coverage'] == coverage_link\n",
    "\n",
    "        df.loc[matching_rows, 'longitude'] = lat_long_df.loc[i, 'lon']\n",
    "        df.loc[matching_rows, 'latitude'] = lat_long_df.loc[i, 'lat']\n",
    "        \n",
    "    df.to_csv(file_path, index=False)\n",
    "    print(f'Updated {filename} with longitude and latitude data.')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2c2afe0079eead43",
   "metadata": {},
   "source": [
    "5) Map the data with folium.\n",
    "\n",
    "I spent way too much time on this. But it works! And it's customized so the cluster icons are dark red."
   ]
  },
  {
   "cell_type": "code",
   "id": "1d48c2c1c1c03fa3",
   "metadata": {},
   "source": [
    "output_directory = '/Users/matthewkollmer/PycharmProjects/nlp_research/vrt_work/lynch_cluster_maps/'\n",
    "\n",
    "def build_maps(file_path):\n",
    "    file_name = os.path.basename(file_path)\n",
    "    \n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    df = df.dropna(subset=['latitude', 'longitude'])\n",
    "    \n",
    "    map_center = [39.8283, -98.5795]  # this is roughly the central location of the USA. Googled it.\n",
    "    \n",
    "    folium_map = folium.Map(location=map_center, zoom_start=3, tiles=\"CartoDB positron\")\n",
    "\n",
    "    # I don't actually know javascript. Thanks ChatGPT!\n",
    "    marker_cluster = MarkerCluster(\n",
    "        icon_create_function=\"\"\"\n",
    "        function (cluster) {\n",
    "            var count = cluster.getChildCount();\n",
    "            var size = 'small';\n",
    "            var iconSize = L.point(1, 1);\n",
    "            \n",
    "            var color = 'rgba(139, 0, 0,';\n",
    "            \n",
    "            if (count < 4) {\n",
    "                color += '.4)';\n",
    "            } else if (count < 10) {\n",
    "                color += '.6)';\n",
    "            } else {\n",
    "                color += '1)'; \n",
    "            }\n",
    "            \n",
    "            return L.divIcon({\n",
    "                html: '<div style=\"background-color:' + color + '; color:white;\"><span style=\"color:white;\">' + count + '</span></div>',\n",
    "                className: 'marker-cluster marker-cluster-' + size,\n",
    "                iconSize: iconSize\n",
    "            });\n",
    "        }\n",
    "        \"\"\"\n",
    "    ).add_to(folium_map)\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        latitude = row['latitude']\n",
    "        longitude = row['longitude']\n",
    "        victim = row['victim']\n",
    "        newspaper = row['newspaper']\n",
    "        reprint_date = row['reprint_date']\n",
    "        url = row['URL']\n",
    "        clipping = row['clippings']\n",
    "        \n",
    "        url_hyperlink = f'<a href=\"{url}\" target=\"_blank\">Read the full page</a>'\n",
    "        \n",
    "        popup_text = f'Victim: {victim}<br><br>Newspaper: {newspaper}<br><br>Reprint Date: {reprint_date}<br><br>{url_hyperlink}<br><br>Newspaper Clipping: <br><br>{clipping}'\n",
    "        \n",
    "        folium.CircleMarker(\n",
    "            location=[latitude, longitude],\n",
    "            radius=4,\n",
    "            color='darkred',\n",
    "            popup=folium.Popup(popup_text, max_width=300)\n",
    "        ).add_to(marker_cluster)\n",
    "    \n",
    "    output_html = os.path.join(output_directory, file_name.replace('.csv', '.html'))\n",
    "    folium_map.save(output_html)\n",
    "    print(f'{file_name} map saved to {output_html}')\n",
    "\n",
    "files = [f for f in os.listdir(directory) if f.endswith('.csv')]\n",
    "\n",
    "for file_name in files:\n",
    "    file_path = os.path.join(directory, file_name)\n",
    "    build_maps(file_path)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9527082f79462fa",
   "metadata": {},
   "source": [
    "6) Display the data based on user selection using ipywidgets."
   ]
  },
  {
   "cell_type": "code",
   "id": "ac2c80ab17c1b2e7",
   "metadata": {},
   "source": [
    "map_directory = '/Users/matthewkollmer/PycharmProjects/nlp_research/vrt_work/lynch_cluster_maps/'\n",
    "\n",
    "def open_map_in_browser(file_name):\n",
    "    file_path = os.path.join(map_directory, file_name)\n",
    "    webbrowser.open(f'file://{file_path}')\n",
    "\n",
    "list_of_maps = [f for f in os.listdir(map_directory) if f.endswith('.html')]\n",
    "\n",
    "dropdown = widgets.Dropdown(\n",
    "    options=list_of_maps,\n",
    "    description='Select Map:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "def open_sesame(change):\n",
    "    open_map_in_browser(change['new'])\n",
    "\n",
    "dropdown.observe(open_sesame, names='value')\n",
    "\n",
    "display(dropdown)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9e7ccbd32d651c03",
   "metadata": {},
   "source": [
    "# Iterate over all files in the map directory\n",
    "for map_file_name in os.listdir(map_directory):\n",
    "    # Skip non-HTML files (assuming the maps are saved as .html)\n",
    "    if not map_file_name.endswith('.html'):\n",
    "        continue\n",
    "    \n",
    "    # Print the HTML <option> tag with the actual map file name\n",
    "    print(f'<option value=\"https://matthewkollmer.com/{map_file_name}\">{map_file_name}</option>')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "b4be3d3652094e95",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Building and Refining the Lynch Clusters Dataset\n",
    "\n",
    "The following notebook is the second iteration of scraping ChronAm for instances of victim names. It also includes new steps to enrich the data and find likely reports of racial violence."
   ],
   "id": "7797686862ae17dc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import os"
   ],
   "id": "41b49c4816dbb47f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1) Build the Broader Dataset\n",
    "\n",
    "To build the new dataset, I'm recycling a lot of functions and loops. I'm following all the same steps but with less stringent search terms for Chron Am."
   ],
   "id": "69b8d1585814ef2d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# use the seguin and rigby dataset again\n",
    "\n",
    "df = pd.read_csv('seguin_rigby_data_black_subset.csv')"
   ],
   "id": "4bf7b31093ff40e0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# this is an updated search url function. It builds the ChronAm search for year and following year and phrase text only.\n",
    "\n",
    "def build_chron_am_search(row):\n",
    "    base_url = \"https://chroniclingamerica.loc.gov/search/pages/results/list/\"\n",
    "    date1 = row['year']\n",
    "    date2 = row['year'] + 1\n",
    "    phrasetext = row['victim'].replace(' ', '+')\n",
    "\n",
    "    search_url = (f'{base_url}?date1={date1}&date2={date2}&searchType=advanced&language='\n",
    "                  f'&proxdistance=5&rows=1000&ortext=&proxtext=&phrasetext={phrasetext}'\n",
    "                  f'&andtext=&dateFilterType=yearRange&page=1&sort=date')\n",
    "    \n",
    "    return search_url\n",
    "\n",
    "df['search_url'] = df.apply(build_chron_am_search, axis=1)\n",
    "\n",
    "df"
   ],
   "id": "2aa2c0ab2d971c5b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# saving the seguin and rigby data with the new search url included.\n",
    "\n",
    "df.to_csv('seguin_rigby_data_black_subset_02.csv', index=False, encoding='utf-8')"
   ],
   "id": "5cf726998d6d8ee2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# this Regex pattern will help to scrape urls that direct to search hits\n",
    "# it is used in the scrape_carefully() function below\n",
    "page_pattern = re.compile(r'/lccn/sn\\d+/\\d{4}-\\d{2}-\\d{2}/ed-\\d/seq-\\d+/')\n",
    "\n",
    "# these are presets to help keep track of time it takes to scrape and the request count\n",
    "request_count = 0\n",
    "first_request_time = None"
   ],
   "id": "9c063ded76539f46",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# this function scrapes ChronAm, but keeps track of requests and chills itself to avoid hitting their rate limits\n",
    "def scrape_carefully(url, retries=3):\n",
    "    global request_count, first_request_time\n",
    "    \n",
    "    if request_count == 0:\n",
    "        first_request_time = datetime.now()\n",
    "    \n",
    "    if request_count >= 200:\n",
    "        elapsed_time = datetime.now() - first_request_time\n",
    "        \n",
    "        if elapsed_time < timedelta(minutes=1):\n",
    "            print('Crawl limit reached. Waiting for 5 minutes.')\n",
    "            time.sleep(300)\n",
    "            first_request_time = datetime.now()\n",
    "            request_count = 0\n",
    "    \n",
    "    if request_count > 0 and request_count % 10 == 0:\n",
    "        print('Burst limit reached. Waiting for 10 seconds.')\n",
    "        time.sleep(10)\n",
    "    \n",
    "    for i in range(retries):\n",
    "        response = requests.get(url)\n",
    "        request_count += 1\n",
    "        print(f'Requests made: {request_count}')\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            return response\n",
    "        \n",
    "        elif response.status_code == 429:\n",
    "            print(f'Received 429 error. Sorry ChronAm. Waiting one hour.')\n",
    "            time.sleep(3605) # this is as safe as can be. Consider shortening chill time.\n",
    "            \n",
    "        else:\n",
    "            print(f'Unexpected error for {url}: {response.status_code}')\n",
    "            return None\n",
    "        \n",
    "    return None"
   ],
   "id": "4aeb31d1b56081d5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# this sets the timer to NOW. And you're off to the races!\n",
    "start_time = datetime.now()\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    search_url = row['search_url']\n",
    "    victim_name = row['victim'].replace(' ', '_')\n",
    "    scrape_results = []\n",
    "    \n",
    "    scrape_content = scrape_carefully(search_url)\n",
    "    \n",
    "    if scrape_content is None:\n",
    "        print(f'Retried 3 times but got repeated errors. Skipping search for victim {victim_name}')\n",
    "        continue\n",
    "    \n",
    "    soup = BeautifulSoup(scrape_content.text, 'html.parser')\n",
    "    \n",
    "    results_list = soup.find('ul', class_='results_list')\n",
    "    \n",
    "    if results_list is None:\n",
    "        print(f'No results found for victim {victim_name}')\n",
    "        continue\n",
    "    \n",
    "    matching_links = results_list.find_all('a', href=page_pattern)\n",
    "    \n",
    "    for link in matching_links:\n",
    "        link_text = link.get_text(strip=True)\n",
    "        match = page_pattern.search(link['href'])\n",
    "        if match:\n",
    "            matched_href = match.group()\n",
    "            link_href = f'https://chroniclingamerica.loc.gov{matched_href}ocr/'\n",
    "            scrape_results.append({'Link Title': link_text, 'URL': link_href})\n",
    "    \n",
    "    print(f'Search for victim {victim_name} processed.')\n",
    "    \n",
    "    if scrape_results:\n",
    "        df_results = pd.DataFrame(scrape_results)\n",
    "        csv_filename = f'lynch_clusters_02/{victim_name}.csv'\n",
    "        df_results.to_csv(csv_filename, index=False)\n",
    "        print(f'Results for victim {victim_name} saved to {csv_filename}')\n",
    "\n",
    "# A little added to keep track of total time elapsed\n",
    "end_time = datetime.now()\n",
    "total_elapsed_time = end_time - start_time\n",
    "print(f'Total elapsed time: {total_elapsed_time}')"
   ],
   "id": "a82c011c276acfd7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# it occurs to me that some victim clusters could have more than 1000 instances. The build_chron_am_search function doesn't account for this. It only retrieves 1000 or less pages. So, the following code checks to see if there are any csv files that reach 1000 hits. If so, there's probably more hits and I'll need to iterate over the ['search_url'] column where 'page={}' is set to 1. But rather than code all that, I thought I'd just check if it's worth it.\n",
    "\n",
    "directory = 'lynch_clusters_02'\n",
    "\n",
    "for file_name in os.listdir(directory):\n",
    "\n",
    "    file_path = os.path.join(directory, file_name)\n",
    "        \n",
    "    df = pd.read_csv(file_path)\n",
    "        \n",
    "    if len(df) >= 999:\n",
    "        print(f'{file_name}: {len(df)} rows')\n",
    "        \n",
    "# turns out, two files (george_white.csv and will_rogers.csv) both have more than 1,000 hits. Perhaps cut them out of further analysis? They're probably just too common of names.\n",
    "\n",
    "# I've deleted them manually from the lynch_clusters_02 directory.\n",
    "# I also deleted ben.csv and jim.csv since they are both just one name (too general)."
   ],
   "id": "b2516f2d01d4f5aa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# thought I should check the number of total hits across all the victim csv files\n",
    "\n",
    "total_hits = 0\n",
    "\n",
    "for file_name in os.listdir(directory):\n",
    "    file_path = os.path.join(directory, file_name)\n",
    "        \n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    total_hits += len(df)\n",
    "\n",
    "print(f'Total hits: {total_hits}')\n",
    "\n",
    "# it's 80,569... Oh dear, it's going to take a long time to scrape all those urls..."
   ],
   "id": "b3f4d697b7a2cbcc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# PICK UP HERE â€“ RUN THE LONG BLOCK, EXPECT MAYBE LIKE 9 HOURS RUN TIME",
   "id": "bf3ce4305abbda5a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Be warned. This block will run for hours.\n",
    "\n",
    "request_count = 0\n",
    "start_time = datetime.now()\n",
    "\n",
    "directory = 'lynch_clusters_02/'\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    file_path = os.path.join(directory, filename)\n",
    "        \n",
    "    df = pd.read_csv(file_path)\n",
    "        \n",
    "    newspaper_content = []\n",
    "        \n",
    "    for url in df['URL']:\n",
    "        try:\n",
    "            response = scrape_carefully(url)\n",
    "                \n",
    "            if response and response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                    \n",
    "                p_tags = soup.find_all('p')\n",
    "                p_text = ' '.join([tag.get_text(strip=True) for tag in p_tags])\n",
    "                    \n",
    "                newspaper_content.append(p_text)\n",
    "                    \n",
    "            else:\n",
    "                newspaper_content.append(None)\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f'Error scraping {url}: {e}')\n",
    "            newspaper_content.append(None)\n",
    "        \n",
    "    df['text'] = newspaper_content\n",
    "    # do not forget you lowercased the text in the scraping process\n",
    "    df['text'] = df['text'].str.lower()\n",
    "        \n",
    "    df.to_csv(file_path, index=False)\n",
    "    print(f'Updated {filename} with OCR text.')\n",
    "\n",
    "end_time = datetime.now()\n",
    "total_elapsed_time = end_time - start_time\n",
    "print(f'Total elapsed time: {total_elapsed_time}')"
   ],
   "id": "c7b59454fa94075f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### GETTING SOME ERROR 61 \n",
    "\n",
    "Note to self: copy output and then take all the urls with error 61: max retries exceeded and retry scraping them separately."
   ],
   "id": "d6e0d2dee32cfa12"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2) Refine the broader dataset\n",
    "\n",
    "I've reviewed a number of searches in Chron Am in the new dataset. For some reason, they don't always actually contain their respective victim names. Chron Am search is mysterious... But to account for this, I will iterate over the csv files, checking for the victim names in the 'text' column. If they do not appear, I will remove the file. Actually, nix this. I am going to remove them at a later step."
   ],
   "id": "d5efdccefbf1cbca"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# But I check for victim names:\n",
    "\n",
    "for file_name in os.listdir(directory):\n",
    "    file_path = os.path.join(directory, file_name)\n",
    "        \n",
    "    df = pd.read_csv(file_path)\n",
    "        \n",
    "    df['text'] = df['text'].astype(str)\n",
    "        \n",
    "    victim_name = file_name.replace('.csv', '').replace('_', ' ')\n",
    "        \n",
    "    if df['text'].str.contains(victim_name, na=False).any():\n",
    "        print(f'{file_name} good')\n",
    "        \n",
    "    else:\n",
    "        print(f'{victim_name} not mentioned in {file_name}. Deleting {file_name}')"
   ],
   "id": "6bdf12fa13dcf2a5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# How many csv files remain?\n",
    "all_victims = os.listdir(directory)\n",
    "len(all_victims)\n",
    "\n",
    "# it's 388 victims and corresponding csv files"
   ],
   "id": "f00296975c5aef59",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3) Add the 'Clippings' column again\n",
    "\n",
    "I've made some adjustments here, too. I optimized the fix_names function. I also expanded the size of the clippings to 150 words before and after the victim's name (before it was 100 words). "
   ],
   "id": "5150e8143e5aac8a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# this is a refactoring of this function. I put the loop outside the regex object and simplified the regex pattern so it detects all instances of victim names where there are any number of non-word characters between the first and last name.\n",
    "\n",
    "def fix_names(text, victim_name):\n",
    "\n",
    "    full_name = victim_name.split()\n",
    "    \n",
    "    for i in range(len(full_name) - 1):\n",
    "\n",
    "        pattern = re.compile(r'(' + re.escape(full_name[i]) + r')\\W*(' + re.escape(full_name[i + 1]) + r')')\n",
    "\n",
    "        text = pattern.sub(r' \\1 \\2 ', text)\n",
    "    \n",
    "    return text"
   ],
   "id": "f1b5d1b13425fded",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# okay, I think I fixed it. I needed to add spaces before and after hits from the fix_names() function then use a tokenizer (in this case, nltk's tokenizer) to accurately identify victim names for clippings.\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def newspaper_clippings(text, victim_name, word_radius=150):\n",
    "    text = fix_names(text, victim_name)\n",
    "\n",
    "    pattern = re.compile(re.escape(victim_name))\n",
    "    \n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    all_clippings = []\n",
    "    \n",
    "    for match in pattern.finditer(text):\n",
    "        start_position = match.start()\n",
    "        end_position = match.end()\n",
    "        \n",
    "        start_word_index = len(word_tokenize(text[:start_position]))\n",
    "        end_word_index = len(word_tokenize(text[:end_position]))\n",
    "\n",
    "        clipping_start_index = max(start_word_index - word_radius, 0)\n",
    "        clipping_end_index = min(end_word_index + word_radius, len(words))\n",
    "        \n",
    "        clipping = ' '.join(words[clipping_start_index:clipping_end_index])\n",
    "        \n",
    "        all_clippings.append(clipping)\n",
    "    \n",
    "    # in case of multiple matches, split them with pipe symbol: |\n",
    "    return ' | '.join(all_clippings) if all_clippings else None"
   ],
   "id": "8947c1a6b20b9f6e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# add the clippings\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    file_path = os.path.join(directory, filename)\n",
    "        \n",
    "    df = pd.read_csv(file_path)\n",
    "        \n",
    "    victim_name = filename.replace('.csv', '').replace('_', ' ')\n",
    "        \n",
    "    clippings = []\n",
    "        \n",
    "    for text in df['text']:\n",
    "        if pd.isna(text):\n",
    "            clippings.append(None)\n",
    "        else:\n",
    "            clipping = newspaper_clippings(text, victim_name)\n",
    "            clippings.append(clipping)\n",
    "        \n",
    "    df['clippings'] = clippings\n",
    "        \n",
    "    df.to_csv(file_path, index=False)\n",
    "    print(f'Updated {filename} with clippings.')"
   ],
   "id": "eedddad5c67e260c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4) Count lynching signifier words in 'Clippings'",
   "id": "ad55b4879652e717"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# redundant, but necessary and I should have thought to do this in previous steps, but whatevs. \n",
    "# Save victim names in a new column:\n",
    "\n",
    "for file_name in os.listdir(directory):\n",
    "\n",
    "    file_path = os.path.join(directory, file_name)\n",
    "        \n",
    "    df = pd.read_csv(file_path)\n",
    "        \n",
    "    victim_name = file_name.replace('.csv', '').replace('_', ' ')\n",
    "\n",
    "    df['victim'] = victim_name\n",
    "        \n",
    "    df.to_csv(file_path, index=False)"
   ],
   "id": "82eff71928acd786",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# extract city from seguin_rigby_data_black_subset_02.csv\n",
    "\n",
    "seguin_rigby_df = pd.read_csv('seguin_rigby_data_black_subset_02.csv')\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    file_path = os.path.join(directory, filename)\n",
    "        \n",
    "    df = pd.read_csv(file_path)\n",
    "        \n",
    "    df['city'] = None\n",
    "        \n",
    "    for i, victim in seguin_rigby_df['victim'].items():\n",
    "\n",
    "        matching_victim = df['victim'] == victim\n",
    "            \n",
    "        df.loc[matching_victim, 'city'] = seguin_rigby_df.loc[i, 'city']\n",
    "        \n",
    "    df['city'] = df['city'].str.lower()\n",
    "    \n",
    "    df.to_csv(file_path, index=False)\n",
    "    print(f'Updated {filename} with city.')"
   ],
   "id": "9eff95e581b9d6b1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# extract state from seguin_rigby data\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    file_path = os.path.join(directory, filename)\n",
    "        \n",
    "    df = pd.read_csv(file_path)\n",
    "        \n",
    "    df['state'] = None\n",
    "        \n",
    "    for i, victim in seguin_rigby_df['victim'].items():\n",
    "\n",
    "        matching_victim = df['victim'] == victim\n",
    "            \n",
    "        df.loc[matching_victim, 'state'] = seguin_rigby_df.loc[i, 'state']\n",
    "    \n",
    "    df['state'] = df['state'].str.lower()\n",
    "    \n",
    "    df.to_csv(file_path, index=False)\n",
    "    print(f'Updated {filename} with state.')"
   ],
   "id": "1c12995ef1284279",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# A little step to identify if city name appears in the clippings. Either yes or no results are saved in a new column called 'city_mentioned'\n",
    "for file_name in os.listdir(directory):\n",
    "    file_path = os.path.join(directory, file_name)\n",
    "        \n",
    "    df = pd.read_csv(file_path)\n",
    "        \n",
    "    df['city_mentioned'] = df.apply(lambda row: 'yes' if row['city'] in row['clippings'] else 'no', axis=1)\n",
    "        \n",
    "    df.to_csv(file_path, index=False)\n",
    "        \n",
    "    print(f'city names reviewed in {file_name}')"
   ],
   "id": "7d0d6b71844f0f46",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# counting signal word instances in the 'clippings' column and saving the counts in a new column\n",
    "# this will give me a sense of how likely the clippings around victim names include text about violence\n",
    "\n",
    "# List of signal words. What else should I add to the list?\n",
    "signal_words = ['lynch', 'mob', 'murder', 'posse', 'shot', 'hang', 'negro']\n",
    "\n",
    "for file_name in os.listdir(directory):\n",
    "    file_path = os.path.join(directory, file_name)\n",
    "        \n",
    "    df = pd.read_csv(file_path)\n",
    "        \n",
    "    df['signal_word_count'] = df['clippings'].apply(lambda text: sum(text.count(word) for word in signal_words))\n",
    "        \n",
    "    df.to_csv(file_path, index=False)\n",
    "        \n",
    "    print(f'counted signal words in {file_name}')"
   ],
   "id": "fb7571a98232f659",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# just to review individual csv files.\n",
    "\n",
    "test_df = pd.read_csv('lynch_clusters_02/zachariah_walker.csv')\n",
    "test_df"
   ],
   "id": "53a39d0faa2d4b5a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 5) Narrow the dataset",
   "id": "e7f700e834f18fbf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# I removed rows where there are NaN values in the 'text' columns\n",
    "for file_name in os.listdir(directory):\n",
    "    file_path = os.path.join(directory, file_name)\n",
    "        \n",
    "    df = pd.read_csv(file_path)\n",
    "        \n",
    "    df = df.dropna(subset=['text'])\n",
    "        \n",
    "    df.to_csv(file_path, index=False)"
   ],
   "id": "f46319aa6efc4afd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# to count the number of rows in all the lynch_clusters\n",
    "\n",
    "total_hits = 0\n",
    "\n",
    "for file_name in os.listdir(directory):\n",
    "\n",
    "    file_path = os.path.join(directory, file_name)\n",
    "        \n",
    "    df = pd.read_csv(file_path)\n",
    "        \n",
    "    number_rows = df.shape[0] \n",
    "    total_hits += number_rows\n",
    "\n",
    "print(f'Total hits: {total_hits}')\n",
    "\n",
    "# it's 57,434"
   ],
   "id": "61057a5b4b02a0ff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# here I narrow the data to subsets that are more likely to by lynching reports\n",
    "# I'm going to save these results in a new directory called lynch_clusters_02_refined\n",
    "# the plan is to test city mentioned and signal word rate combos to deduce what thresholds are valid in considering things nearly certain to be considered racial violence reports\n",
    "refined_directory = 'lynch_clusters_02_refined'\n",
    "\n",
    "for file_name in os.listdir(directory):\n",
    "    file_path = os.path.join(directory, file_name)\n",
    "    \n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # This line defines the threshold. In this case, it's city_mentioned 'yes' and/or 'signal_word_counts' as greater than or equal to 3\n",
    "    # in other words, if a row has city_mentioned in the clipping and/or the clipping has 3 or more signal words, it's saved in the new directory\n",
    "    refined_df = df[(df['city_mentioned'] == 'yes') | (df['signal_word_count'] >= 3)]\n",
    "    \n",
    "    refined_file_path = os.path.join(refined_directory, file_name)\n",
    "    refined_df.to_csv(refined_file_path, index=False)\n",
    "    \n",
    "    print(f'Saved refined file: {refined_file_path}')"
   ],
   "id": "9b87dcab8cf081a3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# How many hits do we have in the refined version now? Let's check with this recycled code:\n",
    "total_hits = 0\n",
    "\n",
    "for file_name in os.listdir(refined_directory):\n",
    "\n",
    "    file_path = os.path.join(refined_directory, file_name)\n",
    "        \n",
    "    df = pd.read_csv(file_path)\n",
    "        \n",
    "    number_rows = df.shape[0] \n",
    "    total_hits += number_rows\n",
    "\n",
    "print(f'Total hits: {total_hits}')\n",
    "\n",
    "# it's 10178 hits (city name mentioned in clipping and/or 3 or more signal words in clipping)"
   ],
   "id": "6f9adf9996221258",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "416ed5be66f17ba1",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
